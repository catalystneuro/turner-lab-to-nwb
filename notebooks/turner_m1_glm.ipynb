{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "## GLM Analysis with NeMoS\n",
    "\n",
    "This notebook demonstrates how to fit **Generalized Linear Models (GLMs)** to predict neural spiking activity from kinematic features using [NeMoS](https://nemos.readthedocs.io/) (Neural Models). We reproduce the methodology from the Turner lab Brain 2016 paper: *\"Movement encoding deficits in the motor cortex of parkinsonian macaque monkeys\"* (Pasquereau & Turner, Brain 2016).\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to build design matrices for neural encoding models\n",
    "- Fitting Poisson GLMs with NeMoS\n",
    "- Using JAX's `vmap` for efficient vectorized fitting\n",
    "- Statistical validation via shuffle-based significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### Why GLMs for Neural Data?\n",
    "\n",
    "PETHs (like in the [PETH tutorial](https://github.com/catalystneuro/turner-lab-to-nwb/blob/main/notebooks/turner_m1_peth.ipynb)) show *when* neurons fire relative to events, but don't tell us *why*. GLMs let us ask: **which kinematic features predict spiking?**\n",
    "\n",
    "The model assumes spike counts follow a Poisson distribution:\n",
    "\n",
    "$$\\text{E}[\\text{spike count}] = \\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...)$$\n",
    "\n",
    "Where $x_i$ are kinematic features (direction, position, velocity, etc.) and $\\beta_i$ are learned coefficients. A positive $\\beta$ means higher feature values increase firing rate; negative means they decrease it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import nemos as nmo\n",
    "import numpy as np\n",
    "import pynapple as nap\n",
    "from pynwb import NWBHDF5IO\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import remfile\n",
    "import h5py\n",
    "\n",
    "# Stream NWB file from DANDI\n",
    "dandiset_id = \"001636\"\n",
    "session_id = \"V++v2703++PreMPTP++Depth19880um++19990607\"\n",
    "\n",
    "client = DandiAPIClient()\n",
    "dandiset = client.get_dandiset(dandiset_id)\n",
    "assets = list(dandiset.get_assets())\n",
    "asset = next(a for a in assets if session_id in a.path)\n",
    "print(f\"Streaming: {asset.path}\")\n",
    "\n",
    "# Open remote file\n",
    "url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "file = remfile.File(url)\n",
    "h5_file = h5py.File(file, \"r\")\n",
    "io = NWBHDF5IO(file=h5_file, load_namespaces=True)\n",
    "nwbfile = io.read()\n",
    "print(f\"Session: {nwbfile.session_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Preparing the Data with Pynapple\n",
    "\n",
    "[Pynapple](https://pynapple.org) and [NeMoS](https://nemos.readthedocs.io/) are designed to work together seamlessly:\n",
    "\n",
    "- **Pynapple** handles data loading, time alignment, and preprocessing (trial extraction, perievent alignment, binning)\n",
    "- **NeMoS** handles the statistical modeling (GLM fitting, regularization, scoring)\n",
    "\n",
    "Both libraries are built on NumPy arrays, so data flows naturally between them. Pynapple's `Tsd` and `TsGroup` objects have `.values` attributes that NeMoS accepts directly. This division of labor keeps each library focused: pynapple doesn't need to implement GLMs, and NeMoS doesn't need to handle NWB files or trial alignment.\n",
    "\n",
    "We'll use pynapple to:\n",
    "1. Load NWB data with `nap.NWBFile()`\n",
    "2. Align kinematics and spikes to movement onset with `compute_perievent_continuous()` and `compute_perievent()`\n",
    "3. Count spikes in time windows with `.restrict()` and `.count()`\n",
    "\n",
    "Then pass the resulting NumPy arrays to NeMoS for GLM fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b724ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nap.NWBFile(nwbfile)\n",
    "print(f\"Trials: {len(data['trials'])}, Units: {len(data['units'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "trials = data[\"trials\"]\n",
    "units = data[\"units\"]\n",
    "spikes = units[0]  # First unit\n",
    "\n",
    "# Kinematics (handle different naming conventions)\n",
    "elbow_position = data[\"ElbowAngle\"] if \"ElbowAngle\" in data.keys() else data[\"SpatialSeriesElbowAngle\"]\n",
    "elbow_velocity = data[\"ElbowVelocity\"] if \"ElbowVelocity\" in data.keys() else data[\"TimeSeriesElbowVelocity\"]\n",
    "\n",
    "# Convert to 1D Tsd - pynapple may load these as TsdFrame (2D) depending on the session.\n",
    "# We need 1D Tsd so that .get(t) returns a scalar that broadcasts correctly across trials.\n",
    "elbow_position = nap.Tsd(t=elbow_position.t, d=np.asarray(elbow_position.values).flatten())\n",
    "elbow_velocity = nap.Tsd(t=elbow_velocity.t, d=np.asarray(elbow_velocity.values).flatten())\n",
    "\n",
    "# Compute acceleration using pynapple's derivative method (wraps numpy.gradient)\n",
    "elbow_acceleration = elbow_velocity.derivative()\n",
    "\n",
    "dt = np.median(np.diff(elbow_velocity.t))\n",
    "print(f\"Kinematics sampling rate: {1/dt:.1f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Z-scoring Features\n",
    "\n",
    "GLM coefficients are more interpretable when features are standardized. A coefficient of 0.5 then means: \"a 1 standard deviation increase in this feature multiplies the firing rate by $e^{0.5} \\approx 1.65$.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score kinematics\n",
    "position_z = nap.Tsd(t=elbow_position.t, d=(elbow_position.values - elbow_position.mean()) / elbow_position.std())\n",
    "velocity_z = nap.Tsd(t=elbow_velocity.t, d=(elbow_velocity.values - elbow_velocity.mean()) / elbow_velocity.std())\n",
    "acceleration_z = nap.Tsd(t=elbow_acceleration.t, d=(elbow_acceleration.values - elbow_acceleration.mean()) / elbow_acceleration.std())\n",
    "\n",
    "# Movement onset times and direction\n",
    "movement_onset_times = nap.Ts(trials[\"derived_movement_onset_time\"].values)\n",
    "direction_values = np.array([1.0 if mt == \"flexion\" else -1.0 for mt in trials[\"movement_type\"]])\n",
    "\n",
    "# Reaction time (z-scored)\n",
    "lateral_target_times = trials[\"lateral_target_appearance_time\"].values\n",
    "movement_onset_values = trials[\"derived_movement_onset_time\"].values\n",
    "reaction_times = movement_onset_values - lateral_target_times\n",
    "reaction_times_z = (reaction_times - reaction_times.mean()) / reaction_times.std()\n",
    "\n",
    "print(f\"Trials: {len(trials)} ({(direction_values == 1).sum()} flexion, {(direction_values == -1).sum()} extension)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Building the Design Matrix\n",
    "\n",
    "**The paper's approach:** Pasquereau & Turner (2016) asked whether M1 neurons encode kinematic features differently before vs. after MPTP-induced parkinsonism. To answer this, they fit a **separate GLM for each time point** relative to movement onset, using trials as observations.\n",
    "\n",
    "**Why time-resolved fitting?** Neural encoding is not static. A neuron might encode velocity *before* movement starts (planning) but switch to encoding position *during* movement (feedback). By fitting independent models at each time point, we can track how encoding evolves.\n",
    "\n",
    "**The sliding window approach:**\n",
    "- **Window size**: 200ms (long enough to get reliable spike counts)\n",
    "- **Step size**: 25ms (fine temporal resolution)\n",
    "- **Time range**: -500ms to +800ms relative to movement onset\n",
    "\n",
    "This creates 45 overlapping windows. For each window, we count spikes and sample kinematic features, then fit a GLM predicting spike count from features. The result is a time series of coefficients showing when each feature matters.\n",
    "\n",
    "**Design matrix structure:**\n",
    "For each time window, we need one row per trial with columns for each feature:\n",
    "\n",
    "| Feature | Description | Values |\n",
    "|---------|-------------|--------|\n",
    "| Direction | Movement type (categorical) | +1 (flexion), -1 (extension) |\n",
    "| Position | Elbow angle at window center | z-scored degrees |\n",
    "| Velocity | Angular velocity at window center | z-scored deg/s |\n",
    "| Acceleration | Angular acceleration at window center | z-scored deg/s² |\n",
    "| RT | Reaction time (constant per trial) | z-scored seconds |\n",
    "\n",
    "The first code cell below aligns all signals to movement onset using pynapple's `compute_perievent` functions. The second cell loops through time windows, extracting spike counts (y) and kinematic features (X) to build the full 3D tensor: `(n_time_bins, n_trials, n_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters matching the paper\n",
    "WINDOW_SIZE_MS = 200.0\n",
    "STEP_SIZE_MS = 25.0\n",
    "TIME_RANGE_MS = (-500.0, 800.0)\n",
    "\n",
    "trial_start_s = TIME_RANGE_MS[0] / 1000.0\n",
    "trial_end_s = TIME_RANGE_MS[1] / 1000.0\n",
    "\n",
    "# Align kinematics and spikes to movement onset using pynapple\n",
    "position_aligned = nap.compute_perievent_continuous(position_z, tref=movement_onset_times, minmax=(trial_start_s, trial_end_s))\n",
    "velocity_aligned = nap.compute_perievent_continuous(velocity_z, tref=movement_onset_times, minmax=(trial_start_s, trial_end_s))\n",
    "acceleration_aligned = nap.compute_perievent_continuous(acceleration_z, tref=movement_onset_times, minmax=(trial_start_s, trial_end_s))\n",
    "peth = nap.compute_perievent(timestamps=spikes, tref=movement_onset_times, minmax=(trial_start_s, trial_end_s))\n",
    "\n",
    "print(f\"Aligned position shape: {position_aligned.shape} (time x trials)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time windows\n",
    "starts_ms = np.arange(TIME_RANGE_MS[0], TIME_RANGE_MS[1] - WINDOW_SIZE_MS + STEP_SIZE_MS, STEP_SIZE_MS)\n",
    "ends_ms = starts_ms + WINDOW_SIZE_MS\n",
    "n_time_bins = len(starts_ms)\n",
    "n_trials = len(trials)\n",
    "\n",
    "# Feature names\n",
    "feature_names = [\"Direction\", \"Position\", \"Velocity\", \"Acceleration\", \"RT\"]\n",
    "n_features = len(feature_names)\n",
    "\n",
    "# Build tensors: X is (time_bins, trials, features), y is (time_bins, trials)\n",
    "X = np.zeros((n_time_bins, n_trials, n_features))\n",
    "y = np.zeros((n_time_bins, n_trials))  # spike counts\n",
    "\n",
    "for win_index, (start_ms, end_ms) in enumerate(zip(starts_ms, ends_ms)):\n",
    "    interval = nap.IntervalSet(start=start_ms / 1000.0, end=end_ms / 1000.0)\n",
    "\n",
    "    # Spike counts in this window\n",
    "    spikes_in_window = peth.restrict(interval)\n",
    "    spike_counts = spikes_in_window.count()\n",
    "    y[win_index, :] = spike_counts.values.flatten()\n",
    "\n",
    "    # Kinematic features at window center\n",
    "    t_center = spike_counts.t.item() if hasattr(spike_counts.t, \"item\") else spike_counts.t[0]\n",
    "    X[win_index, :, 0] = direction_values\n",
    "    X[win_index, :, 1] = position_aligned.restrict(interval).get(t_center)\n",
    "    X[win_index, :, 2] = velocity_aligned.restrict(interval).get(t_center)\n",
    "    X[win_index, :, 3] = acceleration_aligned.restrict(interval).get(t_center)\n",
    "    X[win_index, :, 4] = reaction_times_z  # RT is constant across time bins\n",
    "\n",
    "print(f\"Design matrix X: {X.shape} (time_bins, trials, features)\")\n",
    "print(f\"Spike counts y: {y.shape}\")\n",
    "print(f\"Total spikes: {y.sum():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Fitting GLMs with NeMoS\n",
    "\n",
    "Here's where NeMoS shines. Instead of writing loops to fit 45 separate models (one per time bin), we use JAX's `vmap` to vectorize the fitting. This is both faster and cleaner.\n",
    "\n",
    "**Key NeMoS concepts:**\n",
    "\n",
    "- **`nmo.glm.GLM()`** - The model object, which by default uses a Poisson observation model with exponential link function\n",
    "\n",
    "- **`regularizer=\"Ridge\"`** - L2 regularization adds a penalty on large coefficients. This is important when features are correlated (like velocity and acceleration) because without regularization, the optimizer can find many equivalent solutions, leading to unstable or extreme coefficients.\n",
    "\n",
    "- **`regularizer_strength=0.1`** - Controls how strongly we penalize large coefficients. Higher values = more shrinkage toward zero. We use 0.1 as a reasonable default; you could tune this via cross-validation.\n",
    "\n",
    "- **`solver_kwargs`** - Fine-tune the underlying optimizer (JAXopt's gradient descent). The defaults work for most cases, but time-resolved fitting across many windows can hit edge cases:\n",
    "  - **`stepsize=0.001`** - Learning rate for gradient descent. Smaller values converge more slowly but more reliably. The default (0.01) can overshoot and diverge on some time bins.\n",
    "  - **`acceleration=False`** - Disables Nesterov momentum. Acceleration speeds up convergence but can cause oscillations when the loss surface is tricky. Some time bins have very few spikes, creating ill-conditioned problems where acceleration hurts.\n",
    "  - **`maxiter=5000`** - Maximum optimization steps. More iterations ensure convergence even with the smaller stepsize.\n",
    "\n",
    "These conservative settings ensure we get valid coefficients for *all* 45 time bins, not just most of them. Without them, you may see NaN values in certain windows where the optimizer diverged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the GLM\n",
    "# Ridge regularization stabilizes fitting with correlated features\n",
    "# Smaller stepsize and no acceleration help convergence across all time bins\n",
    "model = nmo.glm.GLM(\n",
    "    regularizer=\"Ridge\",\n",
    "    regularizer_strength=0.1,\n",
    "    solver_kwargs={\"stepsize\": 0.001, \"acceleration\": False, \"maxiter\": 5000},\n",
    ")\n",
    "\n",
    "# Initialize parameters\n",
    "weights_init = np.zeros(n_features)\n",
    "intercept_init = np.array([-1.0])\n",
    "\n",
    "# This is the key step: use JAX vmap to fit all time bins in parallel\n",
    "model.instantiate_solver()\n",
    "vmap_fit = jax.vmap(model.solver_run, in_axes=(None, 0, 0))\n",
    "(coefficients, intercepts), _ = vmap_fit((weights_init, intercept_init), X, y)\n",
    "\n",
    "coefficients = np.array(coefficients)\n",
    "intercepts = np.array(intercepts).flatten()\n",
    "\n",
    "print(f\"Coefficients shape: {coefficients.shape} (time_bins x features)\")\n",
    "print(f\"Fitting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Understanding `vmap`\n",
    "\n",
    "Recall that we need to fit 45 separate GLMs, one for each time window. The naive approach would be a Python loop:\n",
    "\n",
    "```python\n",
    "# Slow loop version\n",
    "coefficients = []\n",
    "for t in range(n_time_bins):\n",
    "    model.fit(X[t], y[t])\n",
    "    coefficients.append(model.coef_)\n",
    "```\n",
    "\n",
    "This works, but it is slow. Each iteration has Python overhead, and the fits run sequentially. With 45 time bins and 100 shuffle iterations for significance testing, that is 4,500 model fits.\n",
    "\n",
    "JAX's `vmap` (vectorized map) solves this by transforming a function that operates on single arrays into one that operates on batches. The key line is:\n",
    "\n",
    "```python\n",
    "vmap_fit = jax.vmap(model.solver_run, in_axes=(None, 0, 0))\n",
    "```\n",
    "\n",
    "This says: \"Take `solver_run`, which fits one GLM, and create a new function that applies it across axis 0 of X and y simultaneously.\" The `in_axes=(None, 0, 0)` specifies:\n",
    "- `None`: Keep the initial parameters the same for all fits\n",
    "- `0`: Iterate over the first axis of X (the time bins)\n",
    "- `0`: Iterate over the first axis of y (the time bins)\n",
    "\n",
    "Now instead of 45 sequential Python calls, we have one vectorized call that JAX compiles into efficient parallel code. On a GPU, all 45 fits can literally run in parallel. Even on CPU, the compiled code avoids Python overhead and enables SIMD optimizations.\n",
    "\n",
    "This pattern is essential for time-resolved neural analysis where you fit many models across time, neurons, or cross-validation folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Visualizing the Coefficients\n",
    "\n",
    "Now we can see how each kinematic feature's influence on firing rate evolves through the movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all coefficients\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "colors = [\"tab:green\", \"tab:blue\", \"tab:orange\", \"tab:red\", \"tab:purple\"]\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    ax.plot(starts_ms, coefficients[:, i], color=colors[i], linewidth=3, label=name)\n",
    "\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.5, linewidth=1.5)\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Movement onset\")\n",
    "ax.set_xlabel(\"Time from movement onset (ms)\", fontsize=18, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"GLM Coefficient\", fontsize=18, fontweight=\"bold\")\n",
    "ax.legend(loc=\"upper right\", fontsize=14, frameon=False)\n",
    "ax.set_title(f\"GLM Coefficients Across Time\\n{asset.path}\", fontsize=20, fontweight=\"bold\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14, width=1.5, length=6)\n",
    "ax.grid(True, alpha=0.3, linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Statistical Validation: Shuffle Test\n",
    "\n",
    "A coefficient being non-zero does not mean it is *significant*. The GLM will always find some coefficients that minimize the loss, even if the features have no real relationship to spiking. We need to ask: could we have obtained this coefficient by chance?\n",
    "\n",
    "**The idea behind permutation testing:**\n",
    "\n",
    "The null hypothesis is that there is no relationship between kinematic features and spike counts. If this were true, which trial had which spike count would be arbitrary. Shuffling the spike counts across trials simulates this null world: the features (X) stay the same, but the spike counts (y) are randomly reassigned to different trials.\n",
    "\n",
    "**What \"shuffling trials\" means concretely:**\n",
    "\n",
    "For each time bin, we have spike counts for 20 trials: `y = [3, 5, 2, 8, 1, ...]`. Shuffling means randomly reordering these values: `y_shuffled = [8, 1, 5, 2, 3, ...]`. Now trial 1's features (direction, position, velocity, etc.) are paired with trial 4's spike count.\n",
    "\n",
    "**What information is preserved vs. destroyed:**\n",
    "\n",
    "Shuffling preserves the *marginal* statistics:\n",
    "- Total spike count stays the same\n",
    "- Mean firing rate stays the same  \n",
    "- Variance of spike counts stays the same\n",
    "- Distribution of each feature stays the same\n",
    "\n",
    "What is destroyed is the **joint distribution**, the covariance between features and spikes. Consider this example:\n",
    "\n",
    "| Trial | Direction | Spikes |\n",
    "|-------|-----------|--------|\n",
    "| 1 | Flexion (+1) | 8 |\n",
    "| 2 | Extension (-1) | 2 |\n",
    "| 3 | Flexion (+1) | 7 |\n",
    "| 4 | Extension (-1) | 3 |\n",
    "\n",
    "The GLM learns: \"Flexion trials have ~7.5 spikes, extension trials have ~2.5 spikes, so the direction coefficient is positive.\"\n",
    "\n",
    "After shuffling the spike counts:\n",
    "\n",
    "| Trial | Direction | Spikes (shuffled) |\n",
    "|-------|-----------|--------|\n",
    "| 1 | Flexion (+1) | 3 |\n",
    "| 2 | Extension (-1) | 8 |\n",
    "| 3 | Flexion (+1) | 2 |\n",
    "| 4 | Extension (-1) | 7 |\n",
    "\n",
    "Now the GLM sees no consistent relationship between direction and spike count. The coefficient will be near zero or even reversed.\n",
    "\n",
    "**The procedure (following Pasquereau & Turner, 2016):**\n",
    "\n",
    "The paper states: *\"To test whether individual coefficients were significant, we shuffled spike counts 1000 times across trials and compared actual coefficients to the confidence intervals yielded by shuffling [P = 0.05/(52 independent time bins) to compensate for multiple comparisons].\"*\n",
    "\n",
    "1. Fit the GLM on real data to get actual coefficients\n",
    "2. Shuffle spike counts across trials (independently for each time bin)\n",
    "3. Refit the GLM on shuffled data to get null coefficients\n",
    "4. Repeat steps 2-3 many times (1000 in the paper, 100 here for speed)\n",
    "5. Compute p-value: what fraction of null coefficients are as extreme as the actual coefficient?\n",
    "6. Apply Bonferroni correction for multiple time bins\n",
    "\n",
    "**Why this works:**\n",
    "\n",
    "This is a non-parametric test. We do not assume coefficients follow a normal distribution or any other parametric form. Instead, we empirically construct the null distribution from the data itself. This is particularly valuable for GLMs where the sampling distribution of coefficients can be complex, especially with small sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SHUFFLES = 100  # Use 1000 for publication-quality results\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "null_coeffs = np.zeros((N_SHUFFLES, n_time_bins, n_features))\n",
    "\n",
    "print(f\"Running {N_SHUFFLES} shuffle iterations...\")\n",
    "for i in range(N_SHUFFLES):\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Shuffle {i + 1}/{N_SHUFFLES}\")\n",
    "    \n",
    "    # Shuffle spike counts across trials (independently for each time bin)\n",
    "    y_shuffled = np.array([rng.permutation(y[t]) for t in range(n_time_bins)])\n",
    "    (null_coeffs[i], _), _ = vmap_fit((weights_init, intercept_init), X, y_shuffled)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute two-tailed p-values\n",
    "p_values = np.zeros((n_time_bins, n_features))\n",
    "for t in range(n_time_bins):\n",
    "    for f in range(n_features):\n",
    "        p_values[t, f] = (np.abs(null_coeffs[:, t, f]) >= np.abs(coefficients[t, f])).mean()\n",
    "\n",
    "# Bonferroni correction for multiple time bins\n",
    "bonferroni_threshold = 0.05 / n_time_bins\n",
    "\n",
    "print(f\"Bonferroni-corrected threshold: p < {bonferroni_threshold:.4f}\")\n",
    "print(f\"\\nSignificant time bins per feature:\")\n",
    "for f, name in enumerate(feature_names):\n",
    "    n_sig = (p_values[:, f] < bonferroni_threshold).sum()\n",
    "    print(f\"  {name}: {n_sig}/{n_time_bins} ({100*n_sig/n_time_bins:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Coefficients with Significance Markers\n",
    "\n",
    "Now we can add stars to show which time bins have statistically significant encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coefficients with significance markers\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    ax.plot(starts_ms, coefficients[:, i], color=colors[i], linewidth=3, label=name)\n",
    "    \n",
    "    # Mark significant time bins with stars\n",
    "    sig_mask = p_values[:, i] < bonferroni_threshold\n",
    "    ax.scatter(starts_ms[sig_mask], coefficients[sig_mask, i], \n",
    "               color=colors[i], s=150, marker=\"*\", zorder=5)\n",
    "\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.5, linewidth=1.5)\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "ax.set_xlabel(\"Time from movement onset (ms)\", fontsize=18, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"GLM Coefficient\", fontsize=18, fontweight=\"bold\")\n",
    "ax.legend(loc=\"upper right\", fontsize=14, frameon=False)\n",
    "ax.set_title(f\"GLM Coefficients with Significance (Bonferroni p<{bonferroni_threshold:.4f})\\n{nwbfile.session_id}\", fontsize=20, fontweight=\"bold\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14, width=1.5, length=6)\n",
    "ax.grid(True, alpha=0.3, linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Examining the Null Distribution\n",
    "\n",
    "Let's visualize what the shuffle test actually does. For one time bin, we can compare the actual coefficient to its null distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a time bin around movement onset\n",
    "sample_time_bin = n_time_bins // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, n_features, figsize=(18, 5))\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, feature_names)):\n",
    "    null_dist = null_coeffs[:, sample_time_bin, i]\n",
    "    actual_coef = coefficients[sample_time_bin, i]\n",
    "    \n",
    "    ax.hist(null_dist, bins=30, alpha=0.7, color=colors[i], edgecolor=\"black\", linewidth=1.2)\n",
    "    ax.axvline(actual_coef, color=\"red\", linewidth=3, linestyle=\"--\", label=f\"Actual: {actual_coef:.2f}\")\n",
    "    ax.axvline(-actual_coef, color=\"red\", linewidth=3, linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel(\"Coefficient\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Count\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_title(f\"{name}\\np = {p_values[sample_time_bin, i]:.3f}\", fontsize=16, fontweight=\"bold\")\n",
    "    ax.legend(fontsize=11, frameon=False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12, width=1.5, length=5)\n",
    "\n",
    "fig.suptitle(f\"Null Distributions at t = {starts_ms[sample_time_bin]:.0f} ms\", fontsize=20, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### Model Validation: Pseudo-R²\n",
    "\n",
    "NeMoS also supports cross-validation via pseudo-R². This tells us how well the model generalizes to held-out data, not just how well it fits the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split trials 80/20\n",
    "rng_cv = np.random.default_rng(123)\n",
    "n_train = int(n_trials * 0.8)\n",
    "train_indices = rng_cv.choice(n_trials, n_train, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(n_trials), train_indices)\n",
    "\n",
    "X_train, X_test = X[:, train_indices, :], X[:, test_indices, :]\n",
    "y_train, y_test = y[:, train_indices], y[:, test_indices]\n",
    "\n",
    "# Fit on training data\n",
    "(coeffs_train, intercepts_train), _ = vmap_fit((weights_init, intercept_init), X_train, y_train)\n",
    "\n",
    "# Compute pseudo-R² on test data for each time bin\n",
    "pseudo_r2 = np.zeros(n_time_bins)\n",
    "for t in range(n_time_bins):\n",
    "    model_eval = nmo.glm.GLM()\n",
    "    model_eval.coef_ = coeffs_train[t]\n",
    "    model_eval.intercept_ = intercepts_train[t]\n",
    "    try:\n",
    "        pseudo_r2[t] = model_eval.score(X_test[t], y_test[t], score_type=\"pseudo-r2-Cohen\")\n",
    "    except Exception:\n",
    "        pseudo_r2[t] = np.nan\n",
    "\n",
    "print(f\"Train trials: {n_train}, Test trials: {len(test_indices)}\")\n",
    "print(f\"Mean pseudo-R²: {np.nanmean(pseudo_r2):.4f}\")\n",
    "print(f\"Max pseudo-R²: {np.nanmax(pseudo_r2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pseudo-R² across time\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(starts_ms, pseudo_r2, color=\"tab:purple\", linewidth=3)\n",
    "ax.fill_between(starts_ms, 0, pseudo_r2, alpha=0.3, color=\"tab:purple\", where=(pseudo_r2 > 0))\n",
    "ax.fill_between(starts_ms, 0, pseudo_r2, alpha=0.3, color=\"tab:red\", where=(pseudo_r2 <= 0))\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.5, linewidth=1.5)\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Time from movement onset (ms)\", fontsize=18, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Pseudo-R² (Cohen)\", fontsize=18, fontweight=\"bold\")\n",
    "ax.set_title(f\"Model Generalization: Train/Test Split ({n_train}/{len(test_indices)} trials)\\n{nwbfile.session_id}\", fontsize=20, fontweight=\"bold\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14, width=1.5, length=6)\n",
    "ax.grid(True, alpha=0.3, linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Interpreting Pseudo-R²\n",
    "\n",
    "**Positive R²**: Model predicts held-out data better than a mean-only (null) model.\n",
    "\n",
    "**Negative R²**: Model is *worse* than just predicting the mean firing rate. This often indicates overfitting due to small sample sizes.\n",
    "\n",
    "With only ~20 trials split 80/20, we have just 4 test trials, which is not enough for reliable cross-validation. The paper addressed this by using shuffle-based significance rather than cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Significance Heatmap\n",
    "\n",
    "A compact way to visualize which features are significant at which times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significance heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Log-transform p-values for better visualization\n",
    "p_log = -np.log10(p_values + 1e-10)\n",
    "threshold_log = -np.log10(bonferroni_threshold)\n",
    "\n",
    "im = ax.imshow(p_log.T, aspect=\"auto\", cmap=\"viridis\",\n",
    "               extent=[starts_ms[0], starts_ms[-1], -0.5, n_features - 0.5])\n",
    "for i in range(1, n_features):\n",
    "    ax.axhline(i - 0.5, color=\"white\", linewidth=1)\n",
    "ax.axvline(0, color=\"white\", linestyle=\"--\", linewidth=2, alpha=0.8)\n",
    "\n",
    "ax.set_yticks(range(n_features))\n",
    "ax.set_yticklabels(feature_names, fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Time from movement onset (ms)\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"-log10(p-value)\", fontsize=14, fontweight=\"bold\")\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.ax.axhline(threshold_log, color=\"red\", linewidth=3)\n",
    "\n",
    "ax.set_title(f\"Significance Across Time (brighter = more significant)\\n{nwbfile.session_id}\", fontsize=20, fontweight=\"bold\")\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "ax.tick_params(axis='x', which='major', labelsize=14, width=1.5, length=6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Summary: NeMoS Workflow\n",
    "\n",
    "| Step | What | Code |\n",
    "|------|------|------|\n",
    "| 1 | Create model | `model = nmo.glm.GLM(regularizer=\"Ridge\")` |\n",
    "| 2 | Prepare solver | `model.instantiate_solver()` |\n",
    "| 3 | Vectorize | `vmap_fit = jax.vmap(model.solver_run, in_axes=(None, 0, 0))` |\n",
    "| 4 | Fit all bins | `(coefs, intercepts), _ = vmap_fit(init, X, y)` |\n",
    "| 5 | Evaluate | `model.score(X_test, y_test, score_type=\"pseudo-r2-Cohen\")` |\n",
    "\n",
    "**Key advantages of NeMoS:**\n",
    "- JAX backend enables GPU acceleration and vectorization\n",
    "- Scikit-learn-like API (`.fit()`, `.score()`, `.predict()`)\n",
    "- Built-in regularization options\n",
    "- Compatible with pynapple for neural data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### What This Neuron Encodes\n",
    "\n",
    "Based on the analysis above, this corticostriatal neuron shows:\n",
    "\n",
    "| Feature | Pattern | Interpretation |\n",
    "|---------|---------|----------------|\n",
    "| **Direction** | Positive, sustained | Fires more for flexion movements |\n",
    "| **Position** | Negative trend | Slight preference for flexed positions |\n",
    "| **Velocity** | Weak/transient | Not strongly velocity-tuned |\n",
    "| **Acceleration** | Near zero | Does not encode acceleration |\n",
    "| **RT** | Near zero | Does not encode reaction time |\n",
    "\n",
    "This is a **direction-selective neuron**. It primarily encodes *what* movement is being made (flexion vs extension), not the detailed kinematics of *how* it is being made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turner-lab-to-nwb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
